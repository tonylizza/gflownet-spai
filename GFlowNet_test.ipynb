{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from preconditioner import PreconditionerEnv\n",
    "from policy import ForwardPolicy, BackwardPolicy\n",
    "from gflownet.gflownet import GFlowNet\n",
    "from gflownet.utils import sparse_one_hot\n",
    "from gflownet.utils import trajectory_balance_loss, market_matrix_to_sparse_tensor\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_memory_usage(stage: str):\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"[{stage}] CPU Memory Usage: {mem_info.rss / (1024 ** 2):.2f} MB\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"[{stage}] GPU Memory Usage: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_path = '../pivtol/pivtol.mtx'  # Update this with your file path\n",
    "batch_size = 3\n",
    "num_epochs = 3000\n",
    "lr = 0.00005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run GMRES Without Preconditioner As Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import mmread\n",
    "from scipy.sparse.linalg import gmres, spilu, LinearOperator\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "\n",
    "# Function to load matrix A from .mtx file\n",
    "def load_mtx_file(file_path):\n",
    "    matrix = mmread(file_path)\n",
    "    return csr_matrix(matrix)\n",
    "\n",
    "# Function to load vector b from .mtx file and ensure it is correctly shaped\n",
    "def load_vector_mtx(file_path):\n",
    "    vector = mmread(file_path)  # Load the vector (could be sparse or dense)\n",
    "    \n",
    "    # Check if the loaded data is a sparse matrix, if so convert it to a dense array\n",
    "    if hasattr(vector, \"toarray\"):\n",
    "        vector = vector.toarray()\n",
    "    \n",
    "    # Flatten the array if it's a row or column vector\n",
    "    vector = vector.flatten()\n",
    "    \n",
    "    return vector\n",
    "\n",
    "# Function to solve the system using GMRES with an optional preconditioner\n",
    "def solve_with_gmres(A, b, M=None):\n",
    "    # Ensure b is a 1D array with the same number of rows as A\n",
    "    b = b.flatten()\n",
    "    if b.shape[0] != A.shape[0]:\n",
    "        raise ValueError(f\"Shape mismatch: A is {A.shape}, but b is {b.shape}\")\n",
    "    \n",
    "    # Initial guess (zero vector)\n",
    "    x0 = np.zeros(b.shape)\n",
    "\n",
    "    # Lists to store iteration number and residual norm\n",
    "    residuals = []\n",
    "    \n",
    "    # Callback function to capture residual norm at each iteration\n",
    "    def callback(rk):\n",
    "        residuals.append(rk)\n",
    "    \n",
    "    # Measure computational time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use GMRES to solve the system Ax = b with preconditioner M\n",
    "    x, exitCode = gmres(A, b, x0=x0, M=M, maxiter=1000000, callback=callback)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if exitCode == 0:\n",
    "        print(\"GMRES converged successfully.\")\n",
    "    else:\n",
    "        print(f\"GMRES did not converge. Exit code: {exitCode}\")\n",
    "    \n",
    "    # Number of iterations is the length of the residuals list\n",
    "    num_iterations = len(residuals)\n",
    "    \n",
    "    return x, residuals, num_iterations, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape mismatch: A is (102, 102), but b is (306,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m b \u001b[38;5;241m=\u001b[39m load_vector_mtx(mtx_file_path_b)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Solve the system\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m x_np, res_np, iter_np, time_np \u001b[38;5;241m=\u001b[39m \u001b[43msolve_with_gmres\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Output the solution\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResidual Norm No Preconditioner: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres_np\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 30\u001b[0m, in \u001b[0;36msolve_with_gmres\u001b[0;34m(A, b, M)\u001b[0m\n\u001b[1;32m     28\u001b[0m b \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m b\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape mismatch: A is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but b is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Initial guess (zero vector)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m x0 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(b\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: Shape mismatch: A is (102, 102), but b is (306,)"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "mtx_file_path_A = matrix_path  # Replace with your actual matrix file path\n",
    "mtx_file_path_b = '../pivtol/pivtol_b.mtx'  # Replace with your actual vector file path\n",
    "\n",
    "# Load the vector data as a numpy array\n",
    "b_data = mmread(mtx_file_path_b)\n",
    "\n",
    "# Check if the data is a matrix (should be, given its shape is 102x3)\n",
    "if b_data.ndim == 2 and b_data.shape[1] > 1:\n",
    "    # Extract the first column\n",
    "    b_single = b_data[:, 0]\n",
    "else:\n",
    "    raise ValueError(\"The loaded data is not in the expected format or does not have multiple columns.\")\n",
    "\n",
    "# Verify the shape of the extracted vector\n",
    "b_single.shape, b_single\n",
    "\n",
    "\n",
    "# Load A and b from the .mtx files\n",
    "A = load_mtx_file(mtx_file_path_A)\n",
    "b = b_single\n",
    "\n",
    "# Solve the system\n",
    "x_np, res_np, iter_np, time_np = solve_with_gmres(A, b)\n",
    "\n",
    "# Output the solution\n",
    "print(f\"Residual Norm No Preconditioner: {res_np}\")\n",
    "print(f\"No iterations No Preconditioner: {iter_np}\")\n",
    "print(f\"Elapsed Time No Preconditioner: {time_np}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before Loading Initial Matrix] CPU Memory Usage: 78.45 MB\n",
      "[After Loading Initial Matrix] CPU Memory Usage: 80.70 MB\n"
     ]
    }
   ],
   "source": [
    "log_memory_usage(\"Before Loading Initial Matrix\")\n",
    "\n",
    "# Load the initial matrix from a file\n",
    "original_matrix = market_matrix_to_sparse_tensor(matrix_path)\n",
    "\n",
    "log_memory_usage(\"After Loading Initial Matrix\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Sampling Preconditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in a PyTorch sparse tensor and samples blocks of a certain size, removing a certain ratio.\n",
    "def structured_sampling(matrix, block_size, keep_ratio=0.5):\n",
    "    if not matrix.is_sparse:\n",
    "        raise ValueError(\"Input matrix must be a PyTorch sparse tensor\")\n",
    "\n",
    "    indices = matrix._indices()\n",
    "    values = matrix._values()\n",
    "    n, m = matrix.shape\n",
    "    blocks = []\n",
    "    block_positions = []\n",
    "\n",
    "    for i in range(0, n, block_size):\n",
    "        for j in range(0, m, block_size):\n",
    "            mask = (indices[0] >= i) & (indices[0] < i + block_size) & (indices[1] >= j) & (indices[1] < j + block_size)\n",
    "            block_indices = indices[:, mask]\n",
    "            block_values = values[mask]\n",
    "            if block_indices.size(1) > 0:  # If the block has non-zero elements\n",
    "                # Sort block values and keep the top elements based on keep_ratio\n",
    "                num_non_zeros = block_indices.size(1)\n",
    "                num_to_keep = max(1, int(num_non_zeros * keep_ratio))\n",
    "                _, top_indices = torch.topk(block_values.abs(), num_to_keep)\n",
    "                \n",
    "                reduced_block_indices = block_indices[:, top_indices]\n",
    "                reduced_block_values = block_values[top_indices]\n",
    "\n",
    "                reduced_block_indices[0] -= i\n",
    "                reduced_block_indices[1] -= j\n",
    "                block_size_tensor = torch.Size([block_size, block_size])\n",
    "                blocks.append(torch.sparse_coo_tensor(reduced_block_indices, reduced_block_values, block_size_tensor))\n",
    "                block_positions.append((i, j))\n",
    "\n",
    "    if len(blocks) == 0:\n",
    "        return torch.sparse_coo_tensor(matrix.size())  # Return an empty sparse matrix if no blocks found\n",
    "\n",
    "    block_diag_indices = []\n",
    "    block_diag_values = []\n",
    "\n",
    "    for (block, (i_offset, j_offset)) in zip(blocks, block_positions):\n",
    "        b_indices = block._indices()\n",
    "        b_values = block._values()\n",
    "        b_indices[0] += i_offset\n",
    "        b_indices[1] += j_offset\n",
    "        block_diag_indices.append(b_indices)\n",
    "        block_diag_values.append(b_values)\n",
    "\n",
    "    block_diag_indices = torch.cat(block_diag_indices, dim=1)\n",
    "    block_diag_values = torch.cat(block_diag_values)\n",
    "    sparse_subset = torch.sparse_coo_tensor(block_diag_indices, block_diag_values, (n, m))\n",
    "\n",
    "    return sparse_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_matrix = structured_sampling(original_matrix, 4, 0.75)\n",
    "matrix_size = initial_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming initial_matrix is a PyTorch tensor\n",
    "# Convert the PyTorch tensor to a NumPy array\n",
    "initial_matrix_dense = initial_matrix.to_dense()\n",
    "initial_matrix_np = initial_matrix_dense.detach().cpu().numpy()\n",
    "\n",
    "# Define the matvec function for the LinearOperator\n",
    "def matvec(x):\n",
    "    return initial_matrix_np.dot(x)\n",
    "\n",
    "# Create the LinearOperator using the matvec function\n",
    "M = LinearOperator(shape=A.shape, matvec=matvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMRES converged successfully.\n",
      "GMRES converged successfully.\n",
      "\n",
      "GMRES without preconditioner:\n",
      "Number of iterations: 17\n",
      "Final residual norm: 1.4499262075380584e-06\n",
      "Elapsed time: 0.0016 seconds\n",
      "\n",
      "GMRES with ILU preconditioner:\n",
      "Number of iterations: 26\n",
      "Final residual norm: 1.2296403292993637e-09\n",
      "Elapsed time: 0.0021 seconds\n"
     ]
    }
   ],
   "source": [
    "# Solve the system without preconditioner\n",
    "solution_no_prec, residuals_no_prec, num_iterations_no_prec, elapsed_time_no_prec = solve_with_gmres(A, b)\n",
    "\n",
    "# Solve the system with ILU preconditioner\n",
    "solution_prec, residuals_prec, num_iterations_prec, elapsed_time_prec = solve_with_gmres(A, b, M=M)\n",
    "\n",
    "# Output the results for comparison\n",
    "print(\"\\nGMRES without preconditioner:\")\n",
    "print(f\"Number of iterations: {num_iterations_no_prec}\")\n",
    "print(f\"Final residual norm: {residuals_no_prec[-1]}\")\n",
    "print(f\"Elapsed time: {elapsed_time_no_prec:.4f} seconds\")\n",
    "\n",
    "print(\"\\nGMRES with Sparse preconditioner:\")\n",
    "print(f\"Number of iterations: {num_iterations_prec}\")\n",
    "print(f\"Final residual norm: {residuals_prec[-1]}\")\n",
    "print(f\"Elapsed time: {elapsed_time_prec:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(initial_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and policies\n",
    "env = PreconditionerEnv(matrix_size=matrix_size, initial_matrix=initial_matrix, original_matrix=original_matrix)\n",
    "env.data.edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_features = -1\n",
    "input_dim = 1\n",
    "hidden_dim = 8\n",
    "forward_policy = ForwardPolicy(node_features=node_features, hidden_dim=hidden_dim, num_actions=env.num_actions)\n",
    "#forward_policy = ForwardPolicy(in_channels=node_features, hidden_channels=hidden_dim, out_channels=env.num_actions)\n",
    "backward_policy = BackwardPolicy(input_dim=input_dim, hidden_dim=hidden_dim, num_actions=env.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.data.edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: {param.grad.norm()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Initialize the GFlowNet model\n",
    "model = GFlowNet(forward_policy, backward_policy, env)\n",
    "opt = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "log_memory_usage(\"After Model Initialization\")\n",
    "\n",
    "report_data = pd.DataFrame(columns=['epoch', 'num_actions', 'loss', 'reward'])\n",
    "\n",
    "detailed_report_data = pd.DataFrame(columns=['epoch', 'sample_number', 'num_actions', 'loss', 'reward'])\n",
    "\n",
    "for epoch in (p := tqdm(range(num_epochs))):\n",
    "   #log_memory_usage(f\"Start of Epoch {epoch}\")\n",
    "\n",
    "    model.train()\n",
    "    #opt.zero_grad()\n",
    "\n",
    "    # Initialize the starting states\n",
    "    initial_indices = torch.zeros(batch_size).long()\n",
    "    #s0 = [sparse_one_hot(initial_indices[i:i+1], env.state_dim).float() for i in range(batch_size)]\n",
    "    s0 = [initial_matrix.clone() for _ in range(batch_size)]\n",
    "    #s0 = one_hot(torch.zeros(batch_size).long(), env.state_dim).float()\n",
    "    # Sample final states and log information\n",
    "    s, log = model.sample_states(s0, return_log=True)\n",
    "    \n",
    "    # Calculate the trajectory balance loss\n",
    "    loss = trajectory_balance_loss(log.total_flow,\n",
    "                                    log.rewards,\n",
    "                                    log.fwd_probs,\n",
    "                                    log.back_probs)\n",
    "    \n",
    "    #print(f\"log.total_flow {log.total_flow}\")\n",
    "    #print(f\"log.rewards {log.rewards}\")\n",
    "    #print(f\"log.fwd_probs {log.fwd_probs}\")\n",
    "    #print(f\"log.back_probs {log.back_probs}\")\n",
    "    #print(f\"log._actions shape {len(log._actions)}\")\n",
    "    #print(f\"Loss Calculation: {loss}\")\n",
    "    # Backpropagation and optimization step\n",
    "    loss.backward()\n",
    "    #check_gradients(model)\n",
    "    opt.step()\n",
    "    #named_params = model.named_parameters()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    #Capture data\n",
    "    total_length = len(log._actions)\n",
    "    report_data = report_data.append({'epoch': epoch, 'num_actions': total_length, 'loss': loss.item(), 'reward': log.rewards}, ignore_index=True)\n",
    "\n",
    "        # Capture data for each sample in the batch\n",
    "    for sample_id in range(batch_size):\n",
    "        sum_actions = log._actions.t()[sample_id]\n",
    "        mask_actions = sum_actions != -1\n",
    "        num_actions = mask_actions.sum()\n",
    "        reward = log.rewards[sample_id].item() if isinstance(log.rewards, torch.Tensor) else log.rewards[sample_id]\n",
    "        detailed_report_data = detailed_report_data.append({\n",
    "            'epoch': epoch,\n",
    "            'sample_number': sample_id + 1,  # Sample number within the batch/epoch\n",
    "            'num_actions': num_actions.item(),\n",
    "            'loss': loss.item(),\n",
    "            'reward': reward\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "       tqdm.write(f\"Epoch {epoch} Loss: {loss.item():.3f}, Num_Actions {total_length}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data.to_csv('training_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_report_data.to_csv('detailed_training_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# Extract the data\n",
    "epochs = report_data['epoch'].values\n",
    "num_actions = report_data['num_actions'].values\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Extract the data\n",
    "epochs = report_data['epoch'].values\n",
    "num_actions = report_data['num_actions'].values\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=epochs,\n",
    "    y=num_actions,\n",
    "    z=losses,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=losses,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    text=[f'Epoch: {e}<br>Num Actions: {n}<br>Loss: {l}' for e, n, l in zip(epochs, num_actions, losses)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            title='Epoch',\n",
    "            range=[0, max(epochs) * 1.1]  # Extend the range slightly beyond the max epoch\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Number of Actions'\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            title='Loss'\n",
    "        )\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data\n",
    "epochs = report_data['epoch'].values\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=epochs,\n",
    "    y=losses,\n",
    "    mode='lines+markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue'\n",
    "    ),\n",
    "    text=[f'Epoch: {e}<br>Loss: {l}' for e, l in zip(epochs, losses)],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title='Epoch'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Loss'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    title='Epoch vs Loss'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data\n",
    "epochs = report_data['epoch'].values.reshape(-1, 1)\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Perform linear regression\n",
    "reg = LinearRegression().fit(epochs, losses)\n",
    "slope = reg.coef_[0]\n",
    "intercept = reg.intercept_\n",
    "\n",
    "# Calculate the regression line\n",
    "regression_line = reg.predict(epochs)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=report_data['epoch'],\n",
    "    y=report_data['loss'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue'\n",
    "    ),\n",
    "    name='Loss',\n",
    "    text=[f'Epoch: {e}<br>Loss: {l}' for e, l in zip(report_data['epoch'], report_data['loss'])],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# Add the regression line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=report_data['epoch'],\n",
    "    y=regression_line,\n",
    "    mode='lines',\n",
    "    line=dict(\n",
    "        color='red'\n",
    "    ),\n",
    "    name='Regression Line'\n",
    "))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title='Epoch'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Loss'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    title=f'Epoch vs Loss (Slope: {slope:.4f})'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Print the slope to determine the trend\n",
    "print(f\"The slope of the regression line is {slope:.4f}\")\n",
    "if slope < 0:\n",
    "    print(\"The values are trending down.\")\n",
    "elif slope > 0:\n",
    "    print(\"The values are trending up.\")\n",
    "else:\n",
    "    print(\"The values are constant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for duplicates across columns\n",
    "def find_column_duplicates(tensor, check_value=None):\n",
    "    num_columns = tensor.size(1)\n",
    "    duplicates = {}\n",
    "    check_value_duplicates = {}\n",
    "    \n",
    "    for col in range(num_columns):\n",
    "        seen = set()\n",
    "        col_duplicates = set()\n",
    "        for row in range(tensor.size(0)):\n",
    "            value = tensor[row, col].item()\n",
    "            if value in seen:\n",
    "                col_duplicates.add(value)\n",
    "            seen.add(value)\n",
    "        \n",
    "        if col_duplicates:\n",
    "            duplicates[col] = col_duplicates\n",
    "        \n",
    "        if check_value is not None and check_value in seen:\n",
    "            check_value_duplicates[col] = check_value in col_duplicates\n",
    "    \n",
    "    return duplicates, check_value_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates, is_negative_one_duplicate = find_column_duplicates(log._actions, check_value=-1)\n",
    "print(\"Duplicate values by column:\", duplicates)\n",
    "print(\"Is -1 a duplicate in each column:\", is_negative_one_duplicate)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log._actions.shape)\n",
    "print(log._traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and plot final states\n",
    "s0 = one_hot(torch.zeros(10**4).long(), env.state_dim).float()\n",
    "s = model.sample_states(s0, return_log=False)\n",
    "# Implement your plot function or use another way to visualize the results\n",
    "# plot(s, env, matrix_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
