{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from preconditioner import PreconditionerEnv\n",
    "from policy import ForwardPolicy, BackwardPolicy\n",
    "from gflownet.gflownet import GFlowNet\n",
    "from gflownet.utils import sparse_one_hot\n",
    "from gflownet.utils import trajectory_balance_loss, market_matrix_to_sparse_tensor, log_memory_usage\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'data/hangGlider_3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_path = data_directory + 'hangGlider_3.mtx'  # Update this with your file path\n",
    "batch_size = 3\n",
    "num_epochs = 1000\n",
    "lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import mmread\n",
    "from scipy.sparse.linalg import gmres, spilu, LinearOperator\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "\n",
    "# Function to load matrix A from .mtx file\n",
    "def load_mtx_file(file_path):\n",
    "    matrix = mmread(file_path)\n",
    "    return csr_matrix(matrix)\n",
    "\n",
    "def load_vector_mtx(file_path):\n",
    "    vector = mmread(file_path)  # Load the vector (could be sparse or dense)\n",
    "    \n",
    "    # Check if the loaded data is a sparse matrix, if so convert it to a dense array\n",
    "    if hasattr(vector, \"toarray\"):\n",
    "        vector = vector.toarray()\n",
    "    \n",
    "    # Flatten the array if it's a row or column vector\n",
    "    vector = vector.flatten()\n",
    "    \n",
    "    return vector\n",
    "\n",
    "# Function to solve the system using GMRES with an optional preconditioner\n",
    "def solve_with_gmres(A, b, M=None):\n",
    "    # Ensure b is a 1D array with the same number of rows as A\n",
    "    b = b.flatten()\n",
    "    if b.shape[0] != A.shape[0]:\n",
    "        raise ValueError(f\"Shape mismatch: A is {A.shape}, but b is {b.shape}\")\n",
    "    \n",
    "    # Initial guess (zero vector)\n",
    "    x0 = np.zeros(b.shape)\n",
    "\n",
    "    # Lists to store iteration number and residual norm\n",
    "    residuals = []\n",
    "    \n",
    "    # Callback function to capture residual norm at each iteration\n",
    "    def callback(rk):\n",
    "        residuals.append(rk)\n",
    "    \n",
    "    # Measure computational time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use GMRES to solve the system Ax = b with preconditioner M\n",
    "    x, exitCode = gmres(A, b, x0=x0, M=M, maxiter=10260, callback=callback)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if exitCode == 0:\n",
    "        print(\"GMRES converged successfully.\")\n",
    "    else:\n",
    "        print(f\"GMRES did not converge. Exit code: {exitCode}\")\n",
    "    \n",
    "    # Number of iterations is the length of the residuals list\n",
    "    num_iterations = len(residuals)\n",
    "    \n",
    "    return x, residuals, num_iterations, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "mtx_file_path_A = matrix_path  # Replace with your actual matrix file path\n",
    "mtx_file_path_b = data_directory + 'hangGlider_3_b.mtx'  # Replace with your actual vector file path\n",
    "\n",
    "# Load the vector data as a numpy array\n",
    "#b = mmread(mtx_file_path_b)\n",
    "\n",
    "\n",
    "\n",
    "# Load A and b from the .mtx files\n",
    "A = load_mtx_file(mtx_file_path_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before Loading Initial Matrix] CPU Memory Usage: 207.09 MB\n",
      "[After Loading Initial Matrix] CPU Memory Usage: 211.20 MB\n"
     ]
    }
   ],
   "source": [
    "log_memory_usage(\"Before Loading Initial Matrix\")\n",
    "\n",
    "# Load the initial matrix from a file\n",
    "original_matrix = market_matrix_to_sparse_tensor(matrix_path)\n",
    "\n",
    "log_memory_usage(\"After Loading Initial Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from scipy.sparse.linalg import gmres\n",
    "import tracemalloc  # For tracking memory usage\n",
    "#Sparse ILU to create baseline preconditioner\n",
    "\n",
    "# Compute the ILU factorization\n",
    "ilu = spla.spilu(A)\n",
    "\n",
    "# Define a function to apply the ILU preconditioner\n",
    "M_x = lambda x: ilu.solve(x)\n",
    "\n",
    "# Create a LinearOperator object from the ILU solver function\n",
    "M = spla.LinearOperator(A.shape, M_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch sparse tensor shape: torch.Size([10260, 10260])\n",
      "Number of non-zero elements: 225812\n",
      "Indices: tensor([[    0,     0,     0,  ..., 10259, 10259, 10259],\n",
      "        [ 1383,  1361,  1358,  ...,    20,    17,    16]])\n",
      "Values: tensor([ 1.2660e-01, -5.8691e-04, -5.8392e-01,  ..., -8.6969e-01,\n",
      "         1.7839e+00, -3.8763e-01])\n"
     ]
    }
   ],
   "source": [
    "#Convert SuperLU object into LU sparse tensor\n",
    "# Extract L and U from the ILU factorization (spilu)\n",
    "L = sp.tril(ilu.L, format='csr')  # Lower triangular matrix from ILU\n",
    "U = sp.triu(ilu.U, format='csr')  # Upper triangular matrix from ILU\n",
    "\n",
    "# Multiply L and U to form the combined LU matrix\n",
    "LU = L @ U  # Sparse matrix multiplication to maintain sparsity\n",
    "\n",
    "# Convert the LU matrix to a PyTorch sparse tensor\n",
    "coo = LU.tocoo()  # Convert to COO format for PyTorch compatibility\n",
    "values = coo.data\n",
    "indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "i = torch.LongTensor(indices)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = torch.Size(coo.shape)\n",
    "\n",
    "#Initial matrix to sample for model\n",
    "initial_matrix = torch.sparse_coo_tensor(i, v, shape)\n",
    "\n",
    "# Print information about the PyTorch sparse tensor\n",
    "print(f\"PyTorch sparse tensor shape: {initial_matrix.shape}\")\n",
    "print(f\"Number of non-zero elements: {initial_matrix._nnz()}\")\n",
    "print(f\"Indices: {initial_matrix._indices()}\")\n",
    "print(f\"Values: {initial_matrix._values()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Sampling Preconditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_matrix = structured_sampling(original_matrix, 4, 0.75)\n",
    "matrix_size = initial_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_matrix.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([225812])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment and policies\n",
    "env = PreconditionerEnv(matrix_size=matrix_size, initial_matrix=initial_matrix, original_matrix=initial_matrix)\n",
    "env.data.edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_features = -1\n",
    "input_dim = 1\n",
    "hidden_dim = 4\n",
    "#forward_policy = ForwardPolicy(node_features=node_features, hidden_dim=hidden_dim)\n",
    "forward_policy = ForwardPolicy(node_features=node_features, hidden_dim=hidden_dim, num_actions=env.num_actions)\n",
    "backward_policy = BackwardPolicy(input_dim=input_dim, hidden_dim=hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([225812])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.data.edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10260, 10260])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_matrix.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: {param.grad.norm()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Before Starting Training] CPU Memory Usage: 1311.68 MB\n"
     ]
    }
   ],
   "source": [
    "log_memory_usage(\"Before Starting Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[After Model Initialization] CPU Memory Usage: 1340.25 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sample Iteration 1] CPU Memory Usage: 1362.37 MB\n",
      "[Before Converting States to Data] CPU Memory Usage: 1362.37 MB\n",
      "[After Converting States to Data] CPU Memory Usage: 1362.46 MB\n",
      "[Before Forward Policy Sampling] CPU Memory Usage: 1362.46 MB\n",
      "[Before Defining Data] CPU Memory Usage: 1362.46 MB\n",
      "[Before Defining num_actions] CPU Memory Usage: 1362.46 MB\n",
      "[Before Defining GAT2] CPU Memory Usage: 1362.46 MB\n",
      "[Before 1st Relu] CPU Memory Usage: 1394.38 MB\n",
      "After ReLu GATConv1 x dimensions: torch.float32\n",
      "[Before 2nd Relu] CPU Memory Usage: 854.03 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Initialize the GFlowNet model\n",
    "model = GFlowNet(forward_policy, backward_policy, env)\n",
    "opt = Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.2, patience=10, verbose=True)\n",
    "\n",
    "\n",
    "log_memory_usage(\"After Model Initialization\")\n",
    "\n",
    "report_data = pd.DataFrame(columns=['epoch', 'num_actions', 'loss', 'reward'])\n",
    "\n",
    "detailed_report_data = pd.DataFrame(columns=['epoch', 'sample_number', 'num_actions', 'loss', 'reward'])\n",
    "\n",
    "for epoch in (p := tqdm(range(num_epochs))):\n",
    "    #log_memory_usage(f\"Start of Epoch {epoch}\")\n",
    "\n",
    "    model.train()\n",
    "    #opt.zero_grad()\n",
    "\n",
    "    # Initialize the starting states\n",
    "    initial_indices = torch.zeros(batch_size).long()\n",
    "    #s0 = [sparse_one_hot(initial_indices[i:i+1], env.state_dim).float() for i in range(batch_size)]\n",
    "    s0 = [initial_matrix.clone() for _ in range(batch_size)]\n",
    "    #print(f\"Cloned initial matrix\")\n",
    "    #s0 = one_hot(torch.zeros(batch_size).long(), env.state_dim).float()\n",
    "    # Sample final states and log information\n",
    "    s, log = model.sample_states(s0, return_log=True)\n",
    "    \n",
    "    # Calculate the trajectory balance loss\n",
    "    loss = trajectory_balance_loss(log.total_flow,\n",
    "                                    log.rewards,\n",
    "                                    log.fwd_probs,\n",
    "                                    log.back_probs)\n",
    "    \n",
    "    #print(f\"log.total_flow {log.total_flow}\")\n",
    "    #print(f\"log.rewards {log.rewards}\")\n",
    "    #print(f\"log.fwd_probs {log.fwd_probs}\")\n",
    "    #print(f\"log.back_probs {log.back_probs}\")\n",
    "    #print(f\"log._actions shape {len(log._actions)}\")\n",
    "    #print(f\"Loss Calculation: {loss}\")\n",
    "    # Backpropagation and optimization step\n",
    "    scheduler.step(loss)\n",
    "    loss.backward()\n",
    "    #check_gradients(model)\n",
    "    opt.step()\n",
    "    #named_params = model.named_parameters()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    #Capture data\n",
    "    total_length = len(log._actions)\n",
    "    report_data = report_data.append({'epoch': epoch, 'num_actions': total_length, 'loss': loss.item(), 'reward': log.rewards}, ignore_index=True)\n",
    "\n",
    "        # Capture data for each sample in the batch\n",
    "    for sample_id in range(batch_size):\n",
    "        sum_actions = log._actions.t()[sample_id]\n",
    "        mask_actions = sum_actions != -1\n",
    "        num_actions = mask_actions.sum()\n",
    "        reward = log.rewards[sample_id].item() if isinstance(log.rewards, torch.Tensor) else log.rewards[sample_id]\n",
    "        detailed_report_data = detailed_report_data.append({\n",
    "            'epoch': epoch,\n",
    "            'sample_number': sample_id + 1,  # Sample number within the batch/epoch\n",
    "            'num_actions': num_actions.item(),\n",
    "            'loss': loss.item(),\n",
    "            'reward': reward\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "       tqdm.write(f\"Epoch {epoch} Loss: {loss.item():.3f}, Num_Actions {total_length}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data.to_csv('training_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_report_data.to_csv('detailed_training_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# Extract the data\n",
    "epochs = report_data['epoch'].values\n",
    "num_actions = report_data['num_actions'].values\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Extract the data\n",
    "epochs = report_data['epoch'].values\n",
    "num_actions = report_data['num_actions'].values\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=epochs,\n",
    "    y=num_actions,\n",
    "    z=losses,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=losses,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    text=[f'Epoch: {e}<br>Num Actions: {n}<br>Loss: {l}' for e, n, l in zip(epochs, num_actions, losses)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            title='Epoch',\n",
    "            range=[0, max(epochs) * 1.1]  # Extend the range slightly beyond the max epoch\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Number of Actions'\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            title='Loss'\n",
    "        )\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data\n",
    "epochs = report_data['epoch'].values\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=epochs,\n",
    "    y=losses,\n",
    "    mode='lines+markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue'\n",
    "    ),\n",
    "    text=[f'Epoch: {e}<br>Loss: {l}' for e, l in zip(epochs, losses)],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title='Epoch'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Loss'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    title='Epoch vs Loss'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data\n",
    "epochs = report_data['epoch'].values.reshape(-1, 1)\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Perform linear regression\n",
    "reg = LinearRegression().fit(epochs, losses)\n",
    "slope = reg.coef_[0]\n",
    "intercept = reg.intercept_\n",
    "\n",
    "# Calculate the regression line\n",
    "regression_line = reg.predict(epochs)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=report_data['epoch'],\n",
    "    y=report_data['loss'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue'\n",
    "    ),\n",
    "    name='Loss',\n",
    "    text=[f'Epoch: {e}<br>Loss: {l}' for e, l in zip(report_data['epoch'], report_data['loss'])],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# Add the regression line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=report_data['epoch'],\n",
    "    y=regression_line,\n",
    "    mode='lines',\n",
    "    line=dict(\n",
    "        color='red'\n",
    "    ),\n",
    "    name='Regression Line'\n",
    "))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title='Epoch'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Loss'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    title=f'Epoch vs Loss (Slope: {slope:.4f})'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Print the slope to determine the trend\n",
    "print(f\"The slope of the regression line is {slope:.4f}\")\n",
    "if slope < 0:\n",
    "    print(\"The values are trending down.\")\n",
    "elif slope > 0:\n",
    "    print(\"The values are trending up.\")\n",
    "else:\n",
    "    print(\"The values are constant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for duplicates across columns\n",
    "def find_column_duplicates(tensor, check_value=None):\n",
    "    num_columns = tensor.size(1)\n",
    "    duplicates = {}\n",
    "    check_value_duplicates = {}\n",
    "    \n",
    "    for col in range(num_columns):\n",
    "        seen = set()\n",
    "        col_duplicates = set()\n",
    "        for row in range(tensor.size(0)):\n",
    "            value = tensor[row, col].item()\n",
    "            if value in seen:\n",
    "                col_duplicates.add(value)\n",
    "            seen.add(value)\n",
    "        \n",
    "        if col_duplicates:\n",
    "            duplicates[col] = col_duplicates\n",
    "        \n",
    "        if check_value is not None and check_value in seen:\n",
    "            check_value_duplicates[col] = check_value in col_duplicates\n",
    "    \n",
    "    return duplicates, check_value_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates, is_negative_one_duplicate = find_column_duplicates(log._actions, check_value=-1)\n",
    "print(\"Duplicate values by column:\", duplicates)\n",
    "print(\"Is -1 a duplicate in each column:\", is_negative_one_duplicate)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log._actions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and plot final states\n",
    "s0 = one_hot(torch.zeros(10**4).long(), env.state_dim).float()\n",
    "s = model.sample_states(s0, return_log=False)\n",
    "# Implement your plot function or use another way to visualize the results\n",
    "# plot(s, env, matrix_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
