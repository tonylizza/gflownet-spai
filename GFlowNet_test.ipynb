{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from preconditioner import PreconditionerEnv\n",
    "from policy import ForwardPolicy, BackwardPolicy\n",
    "from gflownet.gflownet import GFlowNet\n",
    "from gflownet.utils import sparse_one_hot\n",
    "from gflownet.utils import trajectory_balance_loss, market_matrix_to_sparse_tensor\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_memory_usage(stage: str):\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"[{stage}] CPU Memory Usage: {mem_info.rss / (1024 ** 2):.2f} MB\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"[{stage}] GPU Memory Usage: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_path = '../hangGlider_3/hangGlider_3.mtx'  # Update this with your file path\n",
    "batch_size = 3\n",
    "num_epochs = 3000\n",
    "lr = 0.00005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run GMRES Without Preconditioner As Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import mmread\n",
    "from scipy.sparse.linalg import gmres, spilu, LinearOperator\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "\n",
    "# Function to load matrix A from .mtx file\n",
    "def load_mtx_file(file_path):\n",
    "    matrix = mmread(file_path)\n",
    "    return csr_matrix(matrix)\n",
    "\n",
    "def load_vector_mtx(file_path):\n",
    "    vector = mmread(file_path)  # Load the vector (could be sparse or dense)\n",
    "    \n",
    "    # Check if the loaded data is a sparse matrix, if so convert it to a dense array\n",
    "    if hasattr(vector, \"toarray\"):\n",
    "        vector = vector.toarray()\n",
    "    \n",
    "    # Flatten the array if it's a row or column vector\n",
    "    vector = vector.flatten()\n",
    "    \n",
    "    return vector\n",
    "\n",
    "# Function to solve the system using GMRES with an optional preconditioner\n",
    "def solve_with_gmres(A, b, M=None):\n",
    "    # Ensure b is a 1D array with the same number of rows as A\n",
    "    b = b.flatten()\n",
    "    if b.shape[0] != A.shape[0]:\n",
    "        raise ValueError(f\"Shape mismatch: A is {A.shape}, but b is {b.shape}\")\n",
    "    \n",
    "    # Initial guess (zero vector)\n",
    "    x0 = np.zeros(b.shape)\n",
    "\n",
    "    # Lists to store iteration number and residual norm\n",
    "    residuals = []\n",
    "    \n",
    "    # Callback function to capture residual norm at each iteration\n",
    "    def callback(rk):\n",
    "        residuals.append(rk)\n",
    "    \n",
    "    # Measure computational time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use GMRES to solve the system Ax = b with preconditioner M\n",
    "    x, exitCode = gmres(A, b, x0=x0, M=M, maxiter=10260, callback=callback)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    if exitCode == 0:\n",
    "        print(\"GMRES converged successfully.\")\n",
    "    else:\n",
    "        print(f\"GMRES did not converge. Exit code: {exitCode}\")\n",
    "    \n",
    "    # Number of iterations is the length of the residuals list\n",
    "    num_iterations = len(residuals)\n",
    "    \n",
    "    return x, residuals, num_iterations, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "mtx_file_path_A = matrix_path  # Replace with your actual matrix file path\n",
    "mtx_file_path_b = '../hangGlider_3/hangGlider_3_b.mtx'  # Replace with your actual vector file path\n",
    "\n",
    "# Load the vector data as a numpy array\n",
    "b = mmread(mtx_file_path_b)\n",
    "\n",
    "\n",
    "\n",
    "# Load A and b from the .mtx files\n",
    "A = load_mtx_file(mtx_file_path_A)\n",
    "\n",
    "# Solve the system\n",
    "x_np, res_np, iter_np, time_np = solve_with_gmres(A, b)\n",
    "\n",
    "# Output the solution\n",
    "print(f\"Residual Norm No Preconditioner: {res_np}\")\n",
    "print(f\"No iterations No Preconditioner: {iter_np}\")\n",
    "print(f\"Elapsed Time No Preconditioner: {time_np}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_memory_usage(\"Before Loading Initial Matrix\")\n",
    "\n",
    "# Load the initial matrix from a file\n",
    "original_matrix = market_matrix_to_sparse_tensor(matrix_path)\n",
    "\n",
    "log_memory_usage(\"After Loading Initial Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spla\n",
    "from scipy.sparse.linalg import gmres\n",
    "import tracemalloc  # For tracking memory usage\n",
    "#Sparse ILU to create baseline preconditioner\n",
    "\n",
    "# Compute the ILU factorization\n",
    "ilu = spla.spilu(A)\n",
    "\n",
    "# Define a function to apply the ILU preconditioner\n",
    "M_x = lambda x: ilu.solve(x)\n",
    "\n",
    "# Create a LinearOperator object from the ILU solver function\n",
    "M = spla.LinearOperator(A.shape, M_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start memory tracking\n",
    "tracemalloc.start()\n",
    "\n",
    "# Capture the initial memory snapshot\n",
    "start_snapshot = tracemalloc.take_snapshot()\n",
    "\n",
    "# Solve the system using GMRES with the ILU preconditioner\n",
    "# Capture the number of iterations and the final residual norm\n",
    "iterations = []\n",
    "def callback(residual):\n",
    "    iterations.append(residual)\n",
    "\n",
    "x, exitCode = gmres(A, b, M=M, callback=callback)\n",
    "\n",
    "# Capture the final memory snapshot\n",
    "end_snapshot = tracemalloc.take_snapshot()\n",
    "\n",
    "# Calculate the memory usage\n",
    "memory_diff = end_snapshot.compare_to(start_snapshot, 'lineno')\n",
    "total_memory_used = sum(stat.size_diff for stat in memory_diff) / (1024**2)  # Convert to MB\n",
    "\n",
    "# Check the results\n",
    "if exitCode == 0:\n",
    "    print(\"GMRES converged successfully.\")\n",
    "else:\n",
    "    print(f\"GMRES did not converge. Exit code: {exitCode}\")\n",
    "\n",
    "# Final residual norm\n",
    "final_residual_norm = np.linalg.norm(b - A.dot(x))\n",
    "print(f\"Final residual norm: {iterations[-1]}\")\n",
    "\n",
    "# Number of iterations\n",
    "num_iterations = len(iterations)\n",
    "print(f\"Number of iterations: {num_iterations}\")\n",
    "\n",
    "# Memory used\n",
    "print(f\"Memory used during computation: {total_memory_used:.2f} MB\")\n",
    "\n",
    "# Stop memory tracking\n",
    "tracemalloc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert SuperLU object into LU sparse tensor\n",
    "# Extract L and U from the ILU factorization (spilu)\n",
    "L = sp.tril(ilu.L, format='csr')  # Lower triangular matrix from ILU\n",
    "U = sp.triu(ilu.U, format='csr')  # Upper triangular matrix from ILU\n",
    "\n",
    "# Multiply L and U to form the combined LU matrix\n",
    "LU = L @ U  # Sparse matrix multiplication to maintain sparsity\n",
    "\n",
    "# Convert the LU matrix to a PyTorch sparse tensor\n",
    "coo = LU.tocoo()  # Convert to COO format for PyTorch compatibility\n",
    "values = coo.data\n",
    "indices = np.vstack((coo.row, coo.col))\n",
    "\n",
    "i = torch.LongTensor(indices)\n",
    "v = torch.FloatTensor(values)\n",
    "shape = torch.Size(coo.shape)\n",
    "\n",
    "#Initial matrix to sample for model\n",
    "initial_matrix = torch.sparse_coo_tensor(i, v, shape)\n",
    "\n",
    "# Print information about the PyTorch sparse tensor\n",
    "print(f\"PyTorch sparse tensor shape: {initial_matrix.shape}\")\n",
    "print(f\"Number of non-zero elements: {initial_matrix._nnz()}\")\n",
    "print(f\"Indices: {initial_matrix._indices()}\")\n",
    "print(f\"Values: {initial_matrix._values()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Sampling Preconditioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes in a PyTorch sparse tensor and samples blocks of a certain size, removing a certain ratio.\n",
    "def structured_sampling(matrix, block_size, keep_ratio=0.5):\n",
    "    if not matrix.is_sparse:\n",
    "        raise ValueError(\"Input matrix must be a PyTorch sparse tensor\")\n",
    "\n",
    "    indices = matrix._indices()\n",
    "    values = matrix._values()\n",
    "    n, m = matrix.shape\n",
    "    blocks = []\n",
    "    block_positions = []\n",
    "\n",
    "    for i in range(0, n, block_size):\n",
    "        for j in range(0, m, block_size):\n",
    "            mask = (indices[0] >= i) & (indices[0] < i + block_size) & (indices[1] >= j) & (indices[1] < j + block_size)\n",
    "            block_indices = indices[:, mask]\n",
    "            block_values = values[mask]\n",
    "            if block_indices.size(1) > 0:  # If the block has non-zero elements\n",
    "                # Sort block values and keep the top elements based on keep_ratio\n",
    "                num_non_zeros = block_indices.size(1)\n",
    "                num_to_keep = max(1, int(num_non_zeros * keep_ratio))\n",
    "                _, top_indices = torch.topk(block_values.abs(), num_to_keep)\n",
    "                \n",
    "                reduced_block_indices = block_indices[:, top_indices]\n",
    "                reduced_block_values = block_values[top_indices]\n",
    "\n",
    "                reduced_block_indices[0] -= i\n",
    "                reduced_block_indices[1] -= j\n",
    "                block_size_tensor = torch.Size([block_size, block_size])\n",
    "                blocks.append(torch.sparse_coo_tensor(reduced_block_indices, reduced_block_values, block_size_tensor))\n",
    "                block_positions.append((i, j))\n",
    "\n",
    "    if len(blocks) == 0:\n",
    "        return torch.sparse_coo_tensor(matrix.size())  # Return an empty sparse matrix if no blocks found\n",
    "\n",
    "    block_diag_indices = []\n",
    "    block_diag_values = []\n",
    "\n",
    "    for (block, (i_offset, j_offset)) in zip(blocks, block_positions):\n",
    "        b_indices = block._indices()\n",
    "        b_values = block._values()\n",
    "        b_indices[0] += i_offset\n",
    "        b_indices[1] += j_offset\n",
    "        block_diag_indices.append(b_indices)\n",
    "        block_diag_values.append(b_values)\n",
    "\n",
    "    block_diag_indices = torch.cat(block_diag_indices, dim=1)\n",
    "    block_diag_values = torch.cat(block_diag_values)\n",
    "    sparse_subset = torch.sparse_coo_tensor(block_diag_indices, block_diag_values, (n, m))\n",
    "\n",
    "    return sparse_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_matrix = structured_sampling(original_matrix, 4, 0.75)\n",
    "matrix_size = initial_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and policies\n",
    "env = PreconditionerEnv(matrix_size=matrix_size, initial_matrix=initial_matrix, original_matrix=original_matrix)\n",
    "env.data.edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 4\u001b[0m forward_policy \u001b[38;5;241m=\u001b[39m \u001b[43mForwardPolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#forward_policy = ForwardPolicy(in_channels=node_features, hidden_channels=hidden_dim, out_channels=env.num_actions)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m backward_policy \u001b[38;5;241m=\u001b[39m BackwardPolicy(input_dim\u001b[38;5;241m=\u001b[39minput_dim, hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, num_actions\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_actions)\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Thesis_Coding/gflownet-spai/policy.py:25\u001b[0m, in \u001b[0;36mForwardPolicy.__init__\u001b[0;34m(self, node_features, hidden_dim, num_actions)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(node_features, hidden_dim)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions \u001b[38;5;241m=\u001b[39m num_actions\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgat2 \u001b[38;5;241m=\u001b[39m \u001b[43mGATv2Conv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhid\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_head\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/torch_geometric/nn/conv/gatv2_conv.py:161\u001b[0m, in \u001b[0;36mGATv2Conv.__init__\u001b[0;34m(self, in_channels, out_channels, heads, concat, negative_slope, dropout, add_self_loops, edge_dim, fill_value, bias, share_weights, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_weights \u001b[38;5;241m=\u001b[39m share_weights\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(in_channels, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_l \u001b[38;5;241m=\u001b[39m \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mweight_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglorot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m share_weights:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_l\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py:115\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_channels, out_channels, bias, weight_initializer, bias_initializer)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py:138\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Resets all learnable parameters of the module.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[43mreset_weight_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_initializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     reset_bias_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_initializer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py:32\u001b[0m, in \u001b[0;36mreset_weight_\u001b[0;34m(weight, in_channels, initializer)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m initializer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglorot\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[43minits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglorot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m initializer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     34\u001b[0m     bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(in_channels)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/torch_geometric/nn/inits.py:33\u001b[0m, in \u001b[0;36mglorot\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor):\n\u001b[1;32m     32\u001b[0m     stdv \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m6.0\u001b[39m \u001b[38;5;241m/\u001b[39m (value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m value\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstdv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "node_features = -1\n",
    "input_dim = 1\n",
    "hidden_dim = 8\n",
    "forward_policy = ForwardPolicy(node_features=node_features, hidden_dim=hidden_dim, num_actions=env.num_actions)\n",
    "#forward_policy = ForwardPolicy(in_channels=node_features, hidden_channels=hidden_dim, out_channels=env.num_actions)\n",
    "backward_policy = BackwardPolicy(input_dim=input_dim, hidden_dim=hidden_dim, num_actions=env.num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.data.edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradients(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param.grad is not None:\n",
    "                print(f\"{name}: {param.grad.norm()}\")\n",
    "            else:\n",
    "                print(f\"{name}: No gradient\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_memory_usage(\"Before Starting Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Initialize the GFlowNet model\n",
    "model = GFlowNet(forward_policy, backward_policy, env)\n",
    "opt = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "log_memory_usage(\"After Model Initialization\")\n",
    "\n",
    "report_data = pd.DataFrame(columns=['epoch', 'num_actions', 'loss', 'reward'])\n",
    "\n",
    "detailed_report_data = pd.DataFrame(columns=['epoch', 'sample_number', 'num_actions', 'loss', 'reward'])\n",
    "\n",
    "for epoch in (p := tqdm(range(num_epochs))):\n",
    "    log_memory_usage(f\"Start of Epoch {epoch}\")\n",
    "\n",
    "    model.train()\n",
    "    #opt.zero_grad()\n",
    "\n",
    "    # Initialize the starting states\n",
    "    initial_indices = torch.zeros(batch_size).long()\n",
    "    #s0 = [sparse_one_hot(initial_indices[i:i+1], env.state_dim).float() for i in range(batch_size)]\n",
    "    s0 = [initial_matrix.clone() for _ in range(batch_size)]\n",
    "    #s0 = one_hot(torch.zeros(batch_size).long(), env.state_dim).float()\n",
    "    # Sample final states and log information\n",
    "    s, log = model.sample_states(s0, return_log=True)\n",
    "    \n",
    "    # Calculate the trajectory balance loss\n",
    "    loss = trajectory_balance_loss(log.total_flow,\n",
    "                                    log.rewards,\n",
    "                                    log.fwd_probs,\n",
    "                                    log.back_probs)\n",
    "    \n",
    "    #print(f\"log.total_flow {log.total_flow}\")\n",
    "    #print(f\"log.rewards {log.rewards}\")\n",
    "    #print(f\"log.fwd_probs {log.fwd_probs}\")\n",
    "    #print(f\"log.back_probs {log.back_probs}\")\n",
    "    #print(f\"log._actions shape {len(log._actions)}\")\n",
    "    #print(f\"Loss Calculation: {loss}\")\n",
    "    # Backpropagation and optimization step\n",
    "    loss.backward()\n",
    "    #check_gradients(model)\n",
    "    opt.step()\n",
    "    #named_params = model.named_parameters()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    #Capture data\n",
    "    total_length = len(log._actions)\n",
    "    report_data = report_data.append({'epoch': epoch, 'num_actions': total_length, 'loss': loss.item(), 'reward': log.rewards}, ignore_index=True)\n",
    "\n",
    "        # Capture data for each sample in the batch\n",
    "    for sample_id in range(batch_size):\n",
    "        sum_actions = log._actions.t()[sample_id]\n",
    "        mask_actions = sum_actions != -1\n",
    "        num_actions = mask_actions.sum()\n",
    "        reward = log.rewards[sample_id].item() if isinstance(log.rewards, torch.Tensor) else log.rewards[sample_id]\n",
    "        detailed_report_data = detailed_report_data.append({\n",
    "            'epoch': epoch,\n",
    "            'sample_number': sample_id + 1,  # Sample number within the batch/epoch\n",
    "            'num_actions': num_actions.item(),\n",
    "            'loss': loss.item(),\n",
    "            'reward': reward\n",
    "        }, ignore_index=True)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "       tqdm.write(f\"Epoch {epoch} Loss: {loss.item():.3f}, Num_Actions {total_length}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_data.to_csv('training_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_report_data.to_csv('detailed_training_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "# Extract the data\n",
    "epochs = report_data['epoch'].values\n",
    "num_actions = report_data['num_actions'].values\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Extract the data\n",
    "epochs = report_data['epoch'].values\n",
    "num_actions = report_data['num_actions'].values\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=epochs,\n",
    "    y=num_actions,\n",
    "    z=losses,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color=losses,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    text=[f'Epoch: {e}<br>Num Actions: {n}<br>Loss: {l}' for e, n, l in zip(epochs, num_actions, losses)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            title='Epoch',\n",
    "            range=[0, max(epochs) * 1.1]  # Extend the range slightly beyond the max epoch\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Number of Actions'\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            title='Loss'\n",
    "        )\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data\n",
    "epochs = report_data['epoch'].values\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=go.Scatter(\n",
    "    x=epochs,\n",
    "    y=losses,\n",
    "    mode='lines+markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue'\n",
    "    ),\n",
    "    text=[f'Epoch: {e}<br>Loss: {l}' for e, l in zip(epochs, losses)],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title='Epoch'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Loss'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    title='Epoch vs Loss'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data\n",
    "epochs = report_data['epoch'].values.reshape(-1, 1)\n",
    "losses = report_data['loss'].values\n",
    "\n",
    "# Perform linear regression\n",
    "reg = LinearRegression().fit(epochs, losses)\n",
    "slope = reg.coef_[0]\n",
    "intercept = reg.intercept_\n",
    "\n",
    "# Calculate the regression line\n",
    "regression_line = reg.predict(epochs)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=report_data['epoch'],\n",
    "    y=report_data['loss'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=5,\n",
    "        color='blue'\n",
    "    ),\n",
    "    name='Loss',\n",
    "    text=[f'Epoch: {e}<br>Loss: {l}' for e, l in zip(report_data['epoch'], report_data['loss'])],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# Add the regression line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=report_data['epoch'],\n",
    "    y=regression_line,\n",
    "    mode='lines',\n",
    "    line=dict(\n",
    "        color='red'\n",
    "    ),\n",
    "    name='Regression Line'\n",
    "))\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        title='Epoch'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Loss'\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    title=f'Epoch vs Loss (Slope: {slope:.4f})'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Print the slope to determine the trend\n",
    "print(f\"The slope of the regression line is {slope:.4f}\")\n",
    "if slope < 0:\n",
    "    print(\"The values are trending down.\")\n",
    "elif slope > 0:\n",
    "    print(\"The values are trending up.\")\n",
    "else:\n",
    "    print(\"The values are constant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for duplicates across columns\n",
    "def find_column_duplicates(tensor, check_value=None):\n",
    "    num_columns = tensor.size(1)\n",
    "    duplicates = {}\n",
    "    check_value_duplicates = {}\n",
    "    \n",
    "    for col in range(num_columns):\n",
    "        seen = set()\n",
    "        col_duplicates = set()\n",
    "        for row in range(tensor.size(0)):\n",
    "            value = tensor[row, col].item()\n",
    "            if value in seen:\n",
    "                col_duplicates.add(value)\n",
    "            seen.add(value)\n",
    "        \n",
    "        if col_duplicates:\n",
    "            duplicates[col] = col_duplicates\n",
    "        \n",
    "        if check_value is not None and check_value in seen:\n",
    "            check_value_duplicates[col] = check_value in col_duplicates\n",
    "    \n",
    "    return duplicates, check_value_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates, is_negative_one_duplicate = find_column_duplicates(log._actions, check_value=-1)\n",
    "print(\"Duplicate values by column:\", duplicates)\n",
    "print(\"Is -1 a duplicate in each column:\", is_negative_one_duplicate)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log._actions.shape)\n",
    "print(log._traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and plot final states\n",
    "s0 = one_hot(torch.zeros(10**4).long(), env.state_dim).float()\n",
    "s = model.sample_states(s0, return_log=False)\n",
    "# Implement your plot function or use another way to visualize the results\n",
    "# plot(s, env, matrix_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
