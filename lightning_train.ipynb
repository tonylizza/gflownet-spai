{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:106: UserWarning: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "  rank_zero_warn(\"You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\")\n",
      "/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/core/optimizer.py:329: RuntimeWarning: The lr scheduler dict contains the key(s) ['monitor'], but the keys will be ignored. You need to call `lr_scheduler.step()` manually in manual optimization.\n",
      "  rank_zero_warn(\n",
      "/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:411: UserWarning: A layer with UninitializedParameter was found. Thus, the total number of parameters detected may be inaccurate.\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name            | Type           | Params\n",
      "---------------------------------------------------\n",
      "0 | forward_policy  | ForwardPolicy  | 2.7 K \n",
      "1 | backward_policy | BackwardPolicy | 2.6 K \n",
      "---------------------------------------------------\n",
      "5.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.4 K     Total params\n",
      "0.021     Total estimated model params size (MB)\n",
      "/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1558: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdce148cad8b43deb70deb96e201d53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonylizza/Documents/Machine_Learning/Thesis_Coding/gflownet-spai/gflownet/dataset.py:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/pytorch-recipe_1670284382755/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  ilu_indices = torch.tensor([ilu_matrix.row, ilu_matrix.col], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 126.84998321533203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:85: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 20. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "2024-10-02 18:08:26.010122: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 107.47666931152344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:85: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 39. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 78.13609313964844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:85: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 24. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 169.61770629882812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:85: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 18. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 64.1220474243164\n",
      "Loss: 54.591793060302734\n",
      "Loss: 155.33145141601562\n",
      "Loss: 165.19158935546875\n",
      "Loss: 163.46910095214844\n",
      "Loss: 68.3184585571289\n",
      "Loss: 162.6433868408203\n",
      "Loss: 88.02932739257812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/Users/tonylizza/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:437: SparseEfficiencyWarning: spilu converted its input to CSC format\n",
      "  warn('spilu converted its input to CSC format',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of b_vector: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32812318921093814\n",
      "0.15179589887931275\n",
      "0.10120974737554654\n",
      "0.09827242672107069\n",
      "0.09660496166578185\n",
      "0.09550885211837341\n",
      "0.06930874396518417\n",
      "0.012729815114667175\n",
      "0.0068508800875989794\n",
      "0.0027690650855343533\n",
      "0.0012317148149761203\n",
      "0.000316327188247506\n",
      "0.0001989472706301368\n",
      "9.043720845232405e-05\n",
      "8.449511459155003e-05\n",
      "3.129417134234208e-05\n",
      "1.0424079916612562e-05\n",
      "1.8747002397253028e-06\n",
      "GMRES converged successfully.\n",
      "GMRES no preconditioner\n",
      "0.0026174187636077376\n",
      "0.0021290864012168892\n",
      "0.0019634946844222364\n",
      "0.001879777410405055\n",
      "0.0013779486768492582\n",
      "0.0013719907206318712\n",
      "0.001370507822265031\n",
      "0.0013402971904138018\n",
      "0.0011809522869048574\n",
      "0.0007974278023325054\n",
      "0.0006200569434583073\n",
      "3.33762528525477e-05\n",
      "7.626650324963723e-06\n",
      "1.737371358856771e-06\n",
      "1.7371487746582079e-06\n",
      "1.0534158322314024e-06\n",
      "1.0531935303641418e-06\n",
      "1.0471187475693089e-06\n",
      "8.559369745298563e-08\n",
      "8.292919254468309e-08\n",
      "8.292919256451806e-08\n",
      "8.292919256346775e-08\n",
      "8.292919256341593e-08\n",
      "8.292919255848151e-08\n",
      "8.292919254818495e-08\n",
      "8.292919254073948e-08\n",
      "8.292919253495924e-08\n",
      "8.292919252517903e-08\n",
      "8.292919240010326e-08\n",
      "8.292919237414959e-08\n",
      "8.29291918829676e-08\n",
      "8.29291918814945e-08\n",
      "8.29291339346369e-08\n",
      "8.292693668438836e-08\n",
      "8.283522684585123e-08\n",
      "8.283286718861905e-08\n",
      "8.262573163436302e-08\n",
      "8.262572915826117e-08\n",
      "8.262375985607076e-08\n",
      "1.4798058872125516e-08\n",
      "1.4798058894155458e-08\n",
      "1.4798058750628478e-08\n",
      "1.4798058750555411e-08\n",
      "1.4798058717927785e-08\n",
      "1.479805871754002e-08\n",
      "1.4798058206376171e-08\n",
      "1.4798058004859339e-08\n",
      "1.4798057606310215e-08\n",
      "1.479805760629918e-08\n",
      "1.4798039998079931e-08\n",
      "1.4797992283885065e-08\n",
      "1.479799122372313e-08\n",
      "1.4788331173266637e-08\n",
      "1.4785390408993123e-08\n",
      "1.4735870619343995e-08\n",
      "1.4698849322205132e-08\n",
      "1.4692825177547594e-08\n",
      "9.355022480792911e-09\n",
      "8.722448173110622e-09\n",
      "1.3797260083582623e-09\n",
      "1.379726010610342e-09\n",
      "1.379726010328389e-09\n",
      "1.3797260101049855e-09\n",
      "1.3797260099347505e-09\n",
      "1.3797260098815673e-09\n",
      "1.3797260098175494e-09\n",
      "1.3797260084112375e-09\n",
      "1.379726008220109e-09\n",
      "1.3797260071685423e-09\n",
      "1.3797260071010065e-09\n",
      "1.3797258173662267e-09\n",
      "1.379700341563391e-09\n",
      "1.3796287694113833e-09\n",
      "1.3659025240222053e-09\n",
      "1.3405346672999694e-09\n",
      "1.338335154724226e-09\n",
      "1.3380682659390243e-09\n",
      "1.0981006956824239e-09\n",
      "1.0876115013155704e-09\n",
      "2.232715991434858e-11\n",
      "2.2327160039723243e-11\n",
      "2.2327160039697022e-11\n",
      "2.232716003952882e-11\n",
      "2.232716003952562e-11\n",
      "2.2327160029531324e-11\n",
      "2.2327160024673632e-11\n",
      "2.2327160016612107e-11\n",
      "2.232716001552119e-11\n",
      "2.23271598731448e-11\n",
      "2.2327159764136787e-11\n",
      "2.2327158749243516e-11\n",
      "2.2327113338051778e-11\n",
      "2.232693254227688e-11\n",
      "2.168850929987554e-11\n",
      "2.168598266208957e-11\n",
      "2.1081814728553587e-11\n",
      "2.1020955023557265e-11\n",
      "1.4137925104919586e-11\n",
      "1.2784287843746132e-11\n",
      "2.5821288000386858e-12\n",
      "2.5821287895295528e-12\n",
      "2.5821287804000456e-12\n",
      "2.582128760235507e-12\n",
      "2.582128752170321e-12\n",
      "2.582128742806367e-12\n",
      "2.582128215159876e-12\n",
      "2.5821281129929966e-12\n",
      "2.5821276906957884e-12\n",
      "2.5821276867446063e-12\n",
      "2.5821249386136185e-12\n",
      "2.582093997770806e-12\n",
      "2.5820721352746763e-12\n",
      "2.5785970166769307e-12\n",
      "2.560298947335003e-12\n",
      "2.446652916214718e-12\n",
      "2.3649803329840225e-12\n",
      "2.348354239071962e-12\n",
      "3.6707974352802145e-13\n",
      "GMRES converged successfully.\n",
      "GMRES orig preconditioner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mfit(model, data_module)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, logger\u001b[38;5;241m=\u001b[39mlogger)  \u001b[38;5;66;03m# You can modify trainer arguments as needed\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:582\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 582\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:624\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    620\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    622\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1061\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1061\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1163\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py:206\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:324\u001b[0m, in \u001b[0;36mFitLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_strategy_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ML2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1305\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1305\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Thesis_Coding/gflownet-spai/gflownet/gflownet.py:242\u001b[0m, in \u001b[0;36mGFlowNet.on_train_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_end\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    This method is called after training is complete.\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m    We use it to run validation after all training epochs are done.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     aggregated_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_validation_results(results)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Manually log the results using logger\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Machine_Learning/Thesis_Coding/gflownet-spai/gflownet/gflownet.py:282\u001b[0m, in \u001b[0;36mGFlowNet.run_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m convert_sparse_idx_to_row_col(i, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatrix_sq_side\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;241m!=\u001b[39m col:\n\u001b[0;32m--> 282\u001b[0m         L_copy\u001b[38;5;241m.\u001b[39mdata[L_copy\u001b[38;5;241m.\u001b[39mrow \u001b[38;5;241m==\u001b[39m row \u001b[38;5;241m&\u001b[39m L_copy\u001b[38;5;241m.\u001b[39mcol \u001b[38;5;241m==\u001b[39m col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    283\u001b[0m         U_copy\u001b[38;5;241m.\u001b[39mdata[U_copy\u001b[38;5;241m.\u001b[39mrow \u001b[38;5;241m==\u001b[39m row \u001b[38;5;241m&\u001b[39m U_copy\u001b[38;5;241m.\u001b[39mcol \u001b[38;5;241m==\u001b[39m col] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m#sampled_matrix = update_edges_and_convert_to_sparse(batch['data'], sampled_trajectory, batch['matrix_sq_side'])\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m#sampled_csr = torch_sparse_to_csr(sampled_matrix, batch['matrix_sq_side'])\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import Trainer\n",
    "from policy import ForwardPolicy, BackwardPolicy\n",
    "from gflownet.gflownet import GFlowNet\n",
    "from gflownet.dataset import MatrixDataModule\n",
    "\n",
    "def main():\n",
    "    # Initialize the data module\n",
    "    matrix_dir = 'data/small_ILU'  # Replace with your actual directory\n",
    "    data_module = MatrixDataModule(matrix_directory=matrix_dir, batch_size=1)\n",
    "                                   \n",
    "    node_features = -1\n",
    "    input_dim = 1\n",
    "    hidden_dim = 4\n",
    "    max_num_actions = 500\n",
    "    back_action_size = -1\n",
    "    #Initialize the model\n",
    "    forward_policy = ForwardPolicy(node_features=node_features, hidden_dim=hidden_dim, max_num_actions=max_num_actions)\n",
    "    backward_policy = BackwardPolicy(input_dim=input_dim, hidden_dim=hidden_dim, max_num_actions=max_num_actions)\n",
    "\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"gflownet\")\n",
    "\n",
    "    model = GFlowNet(forward_policy=forward_policy, backward_policy=backward_policy, no_sampling_batch=2, lr=0.00002)\n",
    "\n",
    "    # Initialize the PyTorch Lightning Trainer\n",
    "    trainer = Trainer(max_epochs=3, logger=logger)  # You can modify trainer arguments as needed\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
